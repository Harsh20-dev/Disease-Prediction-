# -*- coding: utf-8 -*-
"""Disease Prediction and Precautions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aeveEE2-IJOfd6i-FUrQbS7AS-SthJcv
"""
import streamlit as st
import pandas as pd
import pickle
from google.colab import files

uploaded = files.upload()
print(uploaded.keys())

import zipfile, os
import pandas as pd

zip_name = list(uploaded.keys())[0]
print("Using zip file:", zip_name)


extract_dir = "/content/data"
os.makedirs(extract_dir, exist_ok=True)

with zipfile.ZipFile(zip_name, 'r') as z:
    z.extractall(extract_dir)


for root, dirs, files in os.walk(extract_dir):
    print("DIR:", root)
    for f in files:
        print("   ", f)

csv_path = "/content/data/DiseaseAndSymptoms.csv"

df = pd.read_csv(csv_path)
print("Columns:", df.columns.tolist())
df.head()

symptom_cols = [c for c in df.columns if "Symptom" in c]

def combine_symptoms(row):
    syms = [str(row[c]).strip() for c in symptom_cols if pd.notna(row[c])]
    # lower + comma-separated string
    syms = [s.lower() for s in syms if s != "" and s.lower() != "nan"]
    return ", ".join(syms)

df["Symptoms_text"] = df.apply(combine_symptoms, axis=1)

df[["Disease", "Symptoms_text"]].head()

{
  "instruction": "...",
  "input": "fever, headache, body pain",
  "output": "Disease: dengue\nExplanation: ...\nNote: This is not medical advice."
}

import json
import random

records = []

for i, row in df.iterrows():
    disease = str(row["Disease"]).strip()
    symptoms = str(row["Symptoms_text"]).strip()

    if not symptoms or not disease:
        continue

    explanation = (
        f"These symptoms frequently match patterns seen for {disease} "
        f"in the dataset."
    )

    output_text = (
        f"Disease: {disease}\n"
        f"Explanation: {explanation}\n"
        f"Note: This is not medical or diagnostic advice. "
        f"Consult a qualified doctor for any health concerns."
    )

    record = {
        "instruction": "Identify the most likely disease pattern based on the symptoms.",
        "input": symptoms,
        "output": output_text
    }
    records.append(record)

print("Total usable records:", len(records))


random.shuffle(records)
split_idx = int(0.8 * len(records))
train_data = records[:split_idx]
test_data = records[split_idx:]

with open("train.jsonl", "w") as f:
    for r in train_data:
        f.write(json.dumps(r) + "\n")

with open("test.jsonl", "w") as f:
    for r in test_data:
        f.write(json.dumps(r) + "\n")

print("train.jsonl & test.jsonl created.")

from datasets import load_dataset

data_files = {"train": "train.jsonl", "test": "test.jsonl"}
ds = load_dataset("json", data_files=data_files)

def format_example(example):

    return (
        "### Instruction:\n"
        + example["instruction"]
        + "\n\n### Input (Symptoms):\n"
        + example["input"]
        + "\n\n### Output:\n"
        + example["output"]
    )

ds = ds.map(lambda e: {"text": format_example(e)})
print(ds)
ds["train"][0]["text"]

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
)

model.config.use_cache = False

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

import transformers
print(transformers.__version__)

from transformers import TrainingArguments, Trainer

max_length = 512

def tokenize_function(example):
    tokens = tokenizer(
        example["text"],
        max_length=max_length,
        truncation=True,
        padding="max_length",
    )
    # labels should be same as input_ids for causal LM
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_ds = ds.map(tokenize_function, batched=True, remove_columns=ds["train"].column_names)
tokenized_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

training_args = TrainingArguments(
    output_dir="./tinyllama_disease_lora",
    per_device_train_batch_size=2,        # safe for Colab
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=2,
    learning_rate=2e-4,
    logging_steps=20,
    eval_strategy="epoch",
    save_strategy="epoch",
    fp16=True,
    report_to="none",
    lr_scheduler_type="cosine",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["test"],
    tokenizer=tokenizer,
)

trainer

trainer.train()

save_dir = "final_finetuned_tinyllama_disease_lora"
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("Model saved to:", save_dir)

def ask_model(symptom_text: str):
    prompt = (
        "### Instruction:\nIdentify the most likely disease pattern based on the symptoms.\n\n"
        f"### Input (Symptoms):\n{symptom_text}\n\n### Output:\n"
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    ).to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=120,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)


while True:
    user_input = input("\n Enter symptoms (or type 'exit' to stop): ")
    if user_input.lower() in ["exit", "quit", "stop"]:
        print(" Session Ended.")
        break

    response = ask_model(user_input)
    print("\n Model Response:\n")
    print(response)
